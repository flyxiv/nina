{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3bbf8ce8-70f2-4348-93c6-7acf5af319ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0388faa9-ee27-4ca0-aefa-6f918c606eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MNISTForGAN(Dataset):\n",
    "    def __init__(self, root='./data', train=True, transform=None, download=True):\n",
    "        # 원본 MNIST 데이터셋 로드\n",
    "        self.mnist = datasets.MNIST(\n",
    "            root=root,\n",
    "            train=train,\n",
    "            transform=transform,\n",
    "            download=download\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.mnist)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, _ = self.mnist[idx]  # 원래 라벨은 무시\n",
    "        \n",
    "        # 이미지를 입력과 타겟 모두로 반환\n",
    "        return image, image  # (입력, 타겟)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "88e2d508-7ed8-4e3a-bc24-a4997046570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(MNISTForGAN(transform=transforms.ToTensor()),\n",
    "                                          batch_size=4,\n",
    "                                          shuffle=True,\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046392b8-b032-491e-a04b-1e00bd83665d",
   "metadata": {},
   "source": [
    "# Basic GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c03807c0-f03b-4068-80cc-eb3319dd48d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicGAN(torch.nn.Module):\n",
    "    def __init__(self, n_batch, image_size=28):\n",
    "        super(BasicGAN, self).__init__()\n",
    "        self.n_batch = n_batch\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.image_size ** 2, 200),\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "            torch.nn.Linear(200, 100),\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "            torch.nn.Linear(100, 30),\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "        )\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(30, 100),\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "            torch.nn.Linear(100, 200),\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "            torch.nn.Linear(200, self.image_size ** 2),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = x.reshape((self.n_batch, 1, self.image_size, self.image_size))\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = x.reshape((1, self.image_size, self.image_size))\n",
    "        return x\n",
    "\n",
    "model = BasicGAN(n_batch=4, image_size=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f09a2dbe-b7ef-456b-81b2-b29d313fbd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "71bf1659-5e0e-421b-83b4-3d0d4f17727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Util function for training PyTorch Models.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "def train_model(model, train_loader, /, *, optimizer, loss_fn, num_epochs=25):\n",
    "\tfor epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\t\trunning_loss = 0.0\n",
    "\n",
    "\t\tfor i, data in enumerate(train_loader):\n",
    "\t\t\tinputs, labels = data\n",
    "\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\toutputs = model(inputs)\n",
    "\t\t\tloss = loss_fn(outputs, labels)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\trunning_loss += loss.item()\n",
    "\t\t\tif i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "\t\t\t\tprint(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "\t\t\t\trunning_loss = 0.0\n",
    "\n",
    "\tprint('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "49cfbec2-c282-4868-8a31-a8ebd95bfc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.114\n",
      "[1,  4000] loss: 0.074\n",
      "[1,  6000] loss: 0.058\n",
      "[1,  8000] loss: 0.052\n",
      "[1, 10000] loss: 0.048\n",
      "[1, 12000] loss: 0.046\n",
      "[1, 14000] loss: 0.044\n",
      "[2,  2000] loss: 0.042\n",
      "[2,  4000] loss: 0.041\n",
      "[2,  6000] loss: 0.040\n",
      "[2,  8000] loss: 0.039\n",
      "[2, 10000] loss: 0.039\n",
      "[2, 12000] loss: 0.038\n",
      "[2, 14000] loss: 0.038\n",
      "[3,  2000] loss: 0.037\n",
      "[3,  4000] loss: 0.036\n",
      "[3,  6000] loss: 0.036\n",
      "[3,  8000] loss: 0.036\n",
      "[3, 10000] loss: 0.036\n",
      "[3, 12000] loss: 0.035\n",
      "[3, 14000] loss: 0.035\n",
      "[4,  2000] loss: 0.034\n",
      "[4,  4000] loss: 0.034\n",
      "[4,  6000] loss: 0.034\n",
      "[4,  8000] loss: 0.034\n",
      "[4, 10000] loss: 0.034\n",
      "[4, 12000] loss: 0.034\n",
      "[4, 14000] loss: 0.033\n",
      "[5,  2000] loss: 0.033\n",
      "[5,  4000] loss: 0.033\n",
      "[5,  6000] loss: 0.033\n",
      "[5,  8000] loss: 0.033\n",
      "[5, 10000] loss: 0.033\n",
      "[5, 12000] loss: 0.032\n",
      "[5, 14000] loss: 0.033\n",
      "[6,  2000] loss: 0.032\n",
      "[6,  4000] loss: 0.032\n",
      "[6,  6000] loss: 0.032\n",
      "[6,  8000] loss: 0.032\n",
      "[6, 10000] loss: 0.032\n",
      "[6, 12000] loss: 0.032\n",
      "[6, 14000] loss: 0.032\n",
      "[7,  2000] loss: 0.031\n",
      "[7,  4000] loss: 0.031\n",
      "[7,  6000] loss: 0.031\n",
      "[7,  8000] loss: 0.031\n",
      "[7, 10000] loss: 0.031\n",
      "[7, 12000] loss: 0.031\n",
      "[7, 14000] loss: 0.031\n",
      "[8,  2000] loss: 0.031\n",
      "[8,  4000] loss: 0.031\n",
      "[8,  6000] loss: 0.031\n",
      "[8,  8000] loss: 0.031\n",
      "[8, 10000] loss: 0.031\n",
      "[8, 12000] loss: 0.031\n",
      "[8, 14000] loss: 0.030\n",
      "[9,  2000] loss: 0.030\n",
      "[9,  4000] loss: 0.030\n",
      "[9,  6000] loss: 0.030\n",
      "[9,  8000] loss: 0.030\n",
      "[9, 10000] loss: 0.030\n",
      "[9, 12000] loss: 0.030\n",
      "[9, 14000] loss: 0.030\n",
      "[10,  2000] loss: 0.030\n",
      "[10,  4000] loss: 0.030\n",
      "[10,  6000] loss: 0.030\n",
      "[10,  8000] loss: 0.029\n",
      "[10, 10000] loss: 0.030\n",
      "[10, 12000] loss: 0.029\n",
      "[10, 14000] loss: 0.029\n",
      "[11,  2000] loss: 0.029\n",
      "[11,  4000] loss: 0.029\n",
      "[11,  6000] loss: 0.029\n",
      "[11,  8000] loss: 0.029\n",
      "[11, 10000] loss: 0.029\n",
      "[11, 12000] loss: 0.029\n",
      "[11, 14000] loss: 0.029\n",
      "[12,  2000] loss: 0.029\n",
      "[12,  4000] loss: 0.029\n",
      "[12,  6000] loss: 0.029\n",
      "[12,  8000] loss: 0.029\n",
      "[12, 10000] loss: 0.029\n",
      "[12, 12000] loss: 0.029\n",
      "[12, 14000] loss: 0.028\n",
      "[13,  2000] loss: 0.028\n",
      "[13,  4000] loss: 0.028\n",
      "[13,  6000] loss: 0.028\n",
      "[13,  8000] loss: 0.028\n",
      "[13, 10000] loss: 0.028\n",
      "[13, 12000] loss: 0.028\n",
      "[13, 14000] loss: 0.028\n",
      "[14,  2000] loss: 0.028\n",
      "[14,  4000] loss: 0.028\n",
      "[14,  6000] loss: 0.028\n",
      "[14,  8000] loss: 0.028\n",
      "[14, 10000] loss: 0.028\n",
      "[14, 12000] loss: 0.028\n",
      "[14, 14000] loss: 0.028\n",
      "[15,  2000] loss: 0.028\n",
      "[15,  4000] loss: 0.027\n",
      "[15,  6000] loss: 0.028\n",
      "[15,  8000] loss: 0.028\n",
      "[15, 10000] loss: 0.028\n",
      "[15, 12000] loss: 0.028\n",
      "[15, 14000] loss: 0.028\n",
      "[16,  2000] loss: 0.027\n",
      "[16,  4000] loss: 0.027\n",
      "[16,  6000] loss: 0.028\n",
      "[16,  8000] loss: 0.027\n",
      "[16, 10000] loss: 0.027\n",
      "[16, 12000] loss: 0.027\n",
      "[16, 14000] loss: 0.027\n",
      "[17,  2000] loss: 0.027\n",
      "[17,  4000] loss: 0.027\n",
      "[17,  6000] loss: 0.027\n",
      "[17,  8000] loss: 0.027\n",
      "[17, 10000] loss: 0.027\n",
      "[17, 12000] loss: 0.027\n",
      "[17, 14000] loss: 0.027\n",
      "[18,  2000] loss: 0.027\n",
      "[18,  4000] loss: 0.027\n",
      "[18,  6000] loss: 0.027\n",
      "[18,  8000] loss: 0.027\n",
      "[18, 10000] loss: 0.027\n",
      "[18, 12000] loss: 0.027\n",
      "[18, 14000] loss: 0.027\n",
      "[19,  2000] loss: 0.027\n",
      "[19,  4000] loss: 0.027\n",
      "[19,  6000] loss: 0.027\n",
      "[19,  8000] loss: 0.027\n",
      "[19, 10000] loss: 0.027\n",
      "[19, 12000] loss: 0.027\n",
      "[19, 14000] loss: 0.027\n",
      "[20,  2000] loss: 0.026\n",
      "[20,  4000] loss: 0.027\n",
      "[20,  6000] loss: 0.026\n",
      "[20,  8000] loss: 0.026\n",
      "[20, 10000] loss: 0.026\n",
      "[20, 12000] loss: 0.026\n",
      "[20, 14000] loss: 0.026\n",
      "[21,  2000] loss: 0.026\n",
      "[21,  4000] loss: 0.026\n",
      "[21,  6000] loss: 0.026\n",
      "[21,  8000] loss: 0.026\n",
      "[21, 10000] loss: 0.026\n",
      "[21, 12000] loss: 0.026\n",
      "[21, 14000] loss: 0.026\n",
      "[22,  2000] loss: 0.026\n",
      "[22,  4000] loss: 0.026\n",
      "[22,  6000] loss: 0.026\n",
      "[22,  8000] loss: 0.026\n",
      "[22, 10000] loss: 0.026\n",
      "[22, 12000] loss: 0.026\n",
      "[22, 14000] loss: 0.026\n",
      "[23,  2000] loss: 0.026\n",
      "[23,  4000] loss: 0.026\n",
      "[23,  6000] loss: 0.026\n",
      "[23,  8000] loss: 0.026\n",
      "[23, 10000] loss: 0.026\n",
      "[23, 12000] loss: 0.026\n",
      "[23, 14000] loss: 0.026\n",
      "[24,  2000] loss: 0.026\n",
      "[24,  4000] loss: 0.026\n",
      "[24,  6000] loss: 0.026\n",
      "[24,  8000] loss: 0.025\n",
      "[24, 10000] loss: 0.026\n",
      "[24, 12000] loss: 0.026\n",
      "[24, 14000] loss: 0.026\n",
      "[25,  2000] loss: 0.025\n",
      "[25,  4000] loss: 0.025\n",
      "[25,  6000] loss: 0.026\n",
      "[25,  8000] loss: 0.025\n",
      "[25, 10000] loss: 0.026\n",
      "[25, 12000] loss: 0.025\n",
      "[25, 14000] loss: 0.025\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.L1Loss()\n",
    "train_model(model, data_loader, optimizer=optimizer, loss_fn=loss_fn, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b99057ea-c82d-4db2-bdd9-a5c358a7d3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "        mnist = datasets.MNIST(\n",
    "            root='./data',\n",
    "            train=False,\n",
    "            transform=transforms.ToTensor(),\n",
    "            download=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b842f436-a27f-43f2-b612-6fe03c322f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(MNISTForGAN(transform=transforms.ToTensor()),\n",
    "                                          batch_size=4,\n",
    "                                          shuffle=True,\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f44df52a-ce54-4bb2-a323-539fd049aa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "for data, label in data_loader:\n",
    "    print(data)\n",
    "    predict = model(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "250f84b8-312e-4ff7-836c-50fcd2485d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = predict[0] \n",
    "import numpy as np\n",
    "image_normalized = (image - image.min()) / (image.max() - image.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d7395bf7-07ee-404b-b9e8-048e8a88f6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5156, 0.7511, 0.5154, 0.3944, 0.0985, 0.7325, 0.2991, 0.6579,\n",
       "          0.3431, 0.6393, 0.4869, 0.3668, 0.3568, 0.4797, 0.6886, 0.4473,\n",
       "          0.4712, 0.0566, 0.3472, 0.5486, 0.5051, 0.5518, 0.6183, 0.7950,\n",
       "          0.7720, 0.4250, 0.2545, 0.5308],\n",
       "         [0.4620, 0.3810, 0.3087, 0.3819, 0.7475, 0.4779, 0.6396, 0.5605,\n",
       "          0.4264, 0.7677, 0.3760, 0.4206, 0.7845, 0.7676, 0.4942, 0.4620,\n",
       "          0.4389, 0.2388, 0.3177, 0.3774, 0.5396, 0.2113, 0.2692, 0.5071,\n",
       "          0.2775, 0.7617, 0.7936, 0.3671],\n",
       "         [0.8745, 0.4771, 0.3568, 0.3244, 0.1755, 0.3903, 0.3208, 0.8022,\n",
       "          0.2940, 0.6072, 0.3904, 0.2204, 0.6245, 0.5994, 0.3626, 0.3686,\n",
       "          0.5585, 0.3525, 0.3675, 0.3761, 0.9227, 0.9151, 0.5200, 0.8239,\n",
       "          0.3269, 0.7454, 0.5039, 0.3925],\n",
       "         [0.3515, 0.3419, 0.5542, 0.6211, 0.7848, 0.4814, 0.5553, 0.5291,\n",
       "          0.6064, 0.4860, 0.7342, 0.3591, 0.4412, 0.6515, 0.9549, 0.2638,\n",
       "          0.2787, 0.7436, 0.4909, 0.2618, 0.6740, 0.6238, 0.5236, 0.6284,\n",
       "          0.7373, 0.2657, 0.4540, 0.2329],\n",
       "         [0.2936, 0.4397, 0.4517, 0.4817, 0.4638, 0.1078, 0.6686, 0.5766,\n",
       "          0.6101, 0.5873, 0.6367, 0.5261, 0.4732, 0.3749, 0.7855, 0.6358,\n",
       "          0.8667, 0.4209, 0.6247, 0.3389, 0.2877, 0.6989, 0.3212, 0.5859,\n",
       "          0.5033, 0.4666, 0.5627, 0.3217],\n",
       "         [0.4256, 0.3395, 0.6100, 0.4976, 0.2585, 0.2357, 0.5945, 0.8315,\n",
       "          0.5767, 0.5336, 0.7693, 0.5624, 0.4034, 0.4722, 0.4415, 0.7577,\n",
       "          0.6379, 0.4594, 0.4894, 1.0000, 0.5382, 0.7057, 0.5770, 0.6360,\n",
       "          0.4249, 0.6263, 0.8672, 0.4795],\n",
       "         [0.5601, 0.3800, 0.3848, 0.6111, 0.3729, 0.5174, 0.6622, 0.2494,\n",
       "          0.4103, 0.6229, 0.0923, 0.5078, 0.4864, 0.2949, 0.3960, 0.3896,\n",
       "          0.4897, 0.8305, 0.3496, 0.8459, 0.5522, 0.6176, 0.6295, 0.6008,\n",
       "          0.6633, 0.6554, 0.4164, 0.6717],\n",
       "         [0.6804, 0.5713, 0.3755, 0.3194, 0.2847, 0.6998, 0.1825, 0.4016,\n",
       "          0.3160, 0.6998, 0.8700, 0.5703, 0.3870, 0.4486, 0.2677, 0.2600,\n",
       "          0.2160, 0.8958, 0.6563, 0.3325, 0.7180, 0.3416, 0.3998, 0.4321,\n",
       "          0.4595, 0.7323, 0.2964, 0.3688],\n",
       "         [0.0420, 0.4034, 0.6379, 0.5943, 0.7086, 0.4135, 0.7888, 0.8041,\n",
       "          0.3872, 0.4682, 0.3879, 0.3401, 0.3688, 0.7036, 0.5132, 0.4974,\n",
       "          0.6190, 0.7418, 0.6298, 0.3635, 0.7421, 0.3128, 0.6450, 0.4338,\n",
       "          0.3548, 0.4062, 0.5915, 0.5419],\n",
       "         [0.2446, 0.5222, 0.1114, 0.4784, 0.8392, 0.2302, 0.8402, 0.4517,\n",
       "          0.5252, 0.4944, 0.6718, 0.6954, 0.8700, 0.5232, 0.7665, 0.4620,\n",
       "          0.5464, 0.3884, 0.6586, 0.4530, 0.5105, 0.8104, 0.6713, 0.3511,\n",
       "          0.3409, 0.5242, 0.5175, 0.2540],\n",
       "         [0.5097, 0.6517, 0.7376, 0.6319, 0.1577, 0.3851, 0.5745, 0.7238,\n",
       "          0.3856, 0.2735, 0.5913, 0.7615, 0.2684, 0.3280, 0.3447, 0.4901,\n",
       "          0.7721, 0.7654, 0.8150, 0.7856, 0.5289, 0.6473, 0.3982, 0.7966,\n",
       "          0.7799, 0.5772, 0.5854, 0.7237],\n",
       "         [0.2355, 0.2378, 0.3557, 0.7345, 0.6943, 0.4747, 0.6298, 0.7183,\n",
       "          0.2168, 0.6959, 0.5221, 0.7722, 0.8018, 0.5513, 0.4241, 0.6716,\n",
       "          0.2910, 0.5430, 0.6738, 0.6229, 0.4158, 0.3311, 0.6777, 0.3817,\n",
       "          0.7526, 0.7114, 0.7067, 0.8145],\n",
       "         [0.6058, 0.6580, 0.6833, 0.6690, 0.8779, 0.3384, 0.6184, 0.4478,\n",
       "          0.3554, 0.5872, 0.3671, 0.4783, 0.4618, 0.6406, 0.5631, 0.5065,\n",
       "          0.4110, 0.7179, 0.1159, 0.7369, 0.5472, 0.4090, 0.2169, 0.4557,\n",
       "          0.3650, 0.4804, 0.3305, 0.2728],\n",
       "         [0.6579, 0.5463, 0.6729, 0.6601, 0.5740, 0.9339, 0.8102, 0.9245,\n",
       "          0.2842, 0.3179, 0.3407, 0.6118, 0.6072, 0.5234, 0.5929, 0.3140,\n",
       "          0.4063, 0.7078, 0.6755, 0.6421, 0.5759, 0.3959, 0.4868, 0.6925,\n",
       "          0.5610, 0.4075, 0.4519, 0.3591],\n",
       "         [0.1565, 0.6979, 0.3424, 0.6357, 0.2984, 0.2981, 0.7557, 0.6356,\n",
       "          0.5058, 0.3352, 0.4399, 0.4318, 0.4918, 0.4098, 0.6316, 0.1652,\n",
       "          0.4636, 0.4418, 0.2932, 0.6044, 0.5267, 0.5632, 0.8578, 0.3946,\n",
       "          0.1766, 0.8361, 0.4631, 0.3642],\n",
       "         [0.5809, 0.4114, 0.2272, 0.6437, 0.5740, 0.6468, 0.4694, 0.2105,\n",
       "          0.6132, 0.6346, 0.3547, 0.4771, 0.5245, 0.2405, 0.7897, 0.6860,\n",
       "          0.7583, 0.6557, 0.3210, 0.1972, 0.4677, 0.5236, 0.3188, 0.7028,\n",
       "          0.7135, 0.3524, 0.6221, 0.5136],\n",
       "         [0.8185, 0.3917, 0.4196, 0.4763, 0.9951, 0.2456, 0.4966, 0.3494,\n",
       "          0.7603, 0.2950, 0.4430, 0.4729, 0.4832, 0.2427, 0.6534, 0.3241,\n",
       "          0.5513, 0.6405, 0.4278, 0.6474, 0.5892, 0.3208, 0.8061, 0.4236,\n",
       "          0.5720, 0.7690, 0.5256, 0.4933],\n",
       "         [0.6151, 0.0000, 0.5555, 0.4041, 0.5265, 0.4947, 0.7185, 0.8504,\n",
       "          0.7655, 0.5597, 0.2316, 0.4466, 0.2436, 0.3641, 0.2795, 0.4572,\n",
       "          0.3649, 0.5500, 0.6293, 0.5356, 0.6250, 0.1928, 0.4009, 0.1819,\n",
       "          0.4210, 0.7657, 0.7341, 0.4093],\n",
       "         [0.4231, 0.2170, 0.4824, 0.4061, 0.6969, 0.4965, 0.6388, 0.3272,\n",
       "          0.7274, 0.7507, 0.4169, 0.4689, 0.4327, 0.8157, 0.6781, 0.7254,\n",
       "          0.2678, 0.2705, 0.5586, 0.7435, 0.4146, 0.6789, 0.6287, 0.2795,\n",
       "          0.5629, 0.6810, 0.2072, 0.7718],\n",
       "         [0.4842, 0.3716, 0.3460, 0.3677, 0.3466, 0.2321, 0.4606, 0.4688,\n",
       "          0.5746, 0.7724, 0.6302, 0.4316, 0.7136, 0.7769, 0.4671, 0.2551,\n",
       "          0.6178, 0.3605, 0.7589, 0.4268, 0.6399, 0.4734, 0.7670, 0.5728,\n",
       "          0.2900, 0.6141, 0.4509, 0.4690],\n",
       "         [0.4703, 0.5562, 0.4950, 0.3931, 0.5086, 0.5204, 0.3442, 0.5855,\n",
       "          0.6341, 0.5938, 0.5131, 0.4676, 0.0508, 0.6824, 0.6557, 0.3192,\n",
       "          0.5069, 0.4941, 0.8678, 0.3896, 0.8351, 0.2128, 0.5047, 0.7192,\n",
       "          0.5711, 0.6780, 0.3021, 0.3423],\n",
       "         [0.2451, 0.6429, 0.5176, 0.1926, 0.6238, 0.7962, 0.3745, 0.3505,\n",
       "          0.5627, 0.4663, 0.7678, 0.4059, 0.1961, 0.5153, 0.5102, 0.4790,\n",
       "          0.5924, 0.8137, 0.5639, 0.4504, 0.5214, 0.5555, 0.3598, 0.3219,\n",
       "          0.2027, 0.7071, 0.8373, 0.8483],\n",
       "         [0.6448, 0.2171, 0.6047, 0.4743, 0.4230, 0.5165, 0.2223, 0.4753,\n",
       "          0.8336, 0.7623, 0.2009, 0.2000, 0.5192, 0.6026, 0.4993, 0.4879,\n",
       "          0.4637, 0.7075, 0.3488, 0.6778, 0.2950, 0.6211, 0.6429, 0.4244,\n",
       "          0.3496, 0.6836, 0.1603, 0.2483],\n",
       "         [0.7400, 0.3105, 0.9144, 0.3517, 0.4410, 0.4517, 0.2280, 0.5367,\n",
       "          0.3972, 0.6693, 0.4763, 0.5139, 0.3993, 0.4320, 0.4859, 0.2657,\n",
       "          0.4680, 0.6176, 0.5614, 0.6387, 0.3818, 0.6529, 0.5316, 0.6394,\n",
       "          0.5633, 0.4073, 0.4860, 0.4495],\n",
       "         [0.4690, 0.2721, 0.4271, 0.6321, 0.2373, 0.5760, 0.7258, 0.6471,\n",
       "          0.4524, 0.6876, 0.4964, 0.2700, 0.8102, 0.5430, 0.5839, 0.4652,\n",
       "          0.6963, 0.4546, 0.4439, 0.5044, 0.4608, 0.5224, 0.5764, 0.4597,\n",
       "          0.6798, 0.5950, 0.4891, 0.5084],\n",
       "         [0.6622, 0.6969, 0.6810, 0.4202, 0.4582, 0.6109, 0.7976, 0.5593,\n",
       "          0.7736, 0.8061, 0.5995, 0.3197, 0.8195, 0.8401, 0.6495, 0.3443,\n",
       "          0.4717, 0.3843, 0.8463, 0.3265, 0.6759, 0.8141, 0.7088, 0.5798,\n",
       "          0.6392, 0.5118, 0.6443, 0.8883],\n",
       "         [0.2406, 0.5012, 0.6411, 0.9558, 0.2742, 0.6392, 0.5973, 0.3911,\n",
       "          0.3674, 0.6940, 0.7057, 0.1396, 0.6861, 0.2213, 0.3241, 0.7082,\n",
       "          0.4759, 0.5919, 0.3623, 0.5218, 0.5566, 0.9821, 0.3968, 0.5242,\n",
       "          0.7150, 0.6918, 0.3001, 0.6525],\n",
       "         [0.1593, 0.7258, 0.8228, 0.4543, 0.2232, 0.4607, 0.4026, 0.3425,\n",
       "          0.4309, 0.6805, 0.5847, 0.4611, 0.7580, 0.6313, 0.7244, 0.3983,\n",
       "          0.6308, 0.7036, 0.6402, 0.7103, 0.2322, 0.3341, 0.7517, 0.2471,\n",
       "          0.0081, 0.6438, 0.6482, 0.5687]]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0ebba272-f1e9-418d-919b-f2902306e400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Obtaining dependency information for matplotlib from https://files.pythonhosted.org/packages/d7/68/0d03098b3feb786cbd494df0aac15b571effda7f7cbdec267e8a8d398c16/matplotlib-3.10.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached matplotlib-3.10.1-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Obtaining dependency information for contourpy>=1.0.1 from https://files.pythonhosted.org/packages/a8/7e/cd93cab453720a5d6cb75588cc17dcdc08fc3484b9de98b885924ff61900/contourpy-1.3.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached contourpy-1.3.1-cp311-cp311-win_amd64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: cycler>=0.10 in e:\\nina\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in e:\\nina\\venv\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in e:\\nina\\venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in e:\\nina\\venv\\lib\\site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\nina\\venv\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in e:\\nina\\venv\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in e:\\nina\\venv\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in e:\\nina\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in e:\\nina\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Using cached matplotlib-3.10.1-cp311-cp311-win_amd64.whl (8.1 MB)\n",
      "Using cached contourpy-1.3.1-cp311-cp311-win_amd64.whl (219 kB)\n",
      "Installing collected packages: contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 matplotlib-3.10.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "387fc274-dbcb-4721-a8e4-837beca734af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4990, 0.5148, 0.4996, 0.4912, 0.4714, 0.5159, 0.4853, 0.5094,\n",
       "          0.4871, 0.5076, 0.4971, 0.4899, 0.4885, 0.4969, 0.5112, 0.4949,\n",
       "          0.4960, 0.4695, 0.4893, 0.5021, 0.4996, 0.5024, 0.5078, 0.5187,\n",
       "          0.5190, 0.4927, 0.4817, 0.4992],\n",
       "         [0.4952, 0.4904, 0.4858, 0.4904, 0.5154, 0.4970, 0.5084, 0.5012,\n",
       "          0.4926, 0.5166, 0.4891, 0.4923, 0.5185, 0.5164, 0.4988, 0.4938,\n",
       "          0.4942, 0.4808, 0.4867, 0.4902, 0.5017, 0.4775, 0.4835, 0.4996,\n",
       "          0.4841, 0.5164, 0.5190, 0.4905],\n",
       "         [0.5244, 0.4963, 0.4892, 0.4865, 0.4762, 0.4905, 0.4859, 0.5180,\n",
       "          0.4834, 0.5056, 0.4933, 0.4813, 0.5071, 0.5048, 0.4908, 0.4899,\n",
       "          0.5029, 0.4899, 0.4896, 0.4907, 0.5276, 0.5248, 0.4993, 0.5217,\n",
       "          0.4887, 0.5167, 0.4987, 0.4899],\n",
       "         [0.4887, 0.4880, 0.5020, 0.5060, 0.5173, 0.4973, 0.5017, 0.5024,\n",
       "          0.5073, 0.4991, 0.5146, 0.4879, 0.4934, 0.5091, 0.5320, 0.4820,\n",
       "          0.4832, 0.5155, 0.4975, 0.4826, 0.5099, 0.5065, 0.5004, 0.5059,\n",
       "          0.5155, 0.4836, 0.4941, 0.4806],\n",
       "         [0.4854, 0.4944, 0.4955, 0.4992, 0.4970, 0.4716, 0.5103, 0.5047,\n",
       "          0.5070, 0.5039, 0.5071, 0.5006, 0.4981, 0.4890, 0.5184, 0.5080,\n",
       "          0.5247, 0.4938, 0.5074, 0.4877, 0.4849, 0.5114, 0.4878, 0.5041,\n",
       "          0.4988, 0.4972, 0.5022, 0.4864],\n",
       "         [0.4932, 0.4875, 0.5056, 0.4981, 0.4806, 0.4801, 0.5042, 0.5217,\n",
       "          0.5028, 0.5025, 0.5168, 0.5018, 0.4928, 0.4965, 0.4940, 0.5169,\n",
       "          0.5067, 0.4963, 0.4969, 0.5330, 0.4999, 0.5140, 0.5045, 0.5080,\n",
       "          0.4935, 0.5086, 0.5232, 0.4960],\n",
       "         [0.5029, 0.4886, 0.4909, 0.5066, 0.4903, 0.4995, 0.5110, 0.4814,\n",
       "          0.4919, 0.5061, 0.4703, 0.4979, 0.4971, 0.4844, 0.4912, 0.4915,\n",
       "          0.4979, 0.5198, 0.4895, 0.5209, 0.5024, 0.5064, 0.5073, 0.5058,\n",
       "          0.5106, 0.5094, 0.4937, 0.5107],\n",
       "         [0.5100, 0.5040, 0.4889, 0.4866, 0.4844, 0.5107, 0.4759, 0.4918,\n",
       "          0.4838, 0.5125, 0.5242, 0.5025, 0.4901, 0.4960, 0.4820, 0.4824,\n",
       "          0.4802, 0.5259, 0.5099, 0.4868, 0.5135, 0.4876, 0.4906, 0.4937,\n",
       "          0.4962, 0.5137, 0.4853, 0.4903],\n",
       "         [0.4675, 0.4910, 0.5078, 0.5039, 0.5125, 0.4932, 0.5176, 0.5197,\n",
       "          0.4916, 0.4949, 0.4911, 0.4880, 0.4891, 0.5116, 0.4991, 0.4968,\n",
       "          0.5062, 0.5151, 0.5064, 0.4882, 0.5136, 0.4867, 0.5088, 0.4950,\n",
       "          0.4895, 0.4908, 0.5041, 0.4999],\n",
       "         [0.4804, 0.4999, 0.4712, 0.4976, 0.5212, 0.4800, 0.5208, 0.4959,\n",
       "          0.4993, 0.4976, 0.5109, 0.5114, 0.5229, 0.5004, 0.5160, 0.4956,\n",
       "          0.4992, 0.4914, 0.5090, 0.4947, 0.4991, 0.5193, 0.5103, 0.4893,\n",
       "          0.4889, 0.5016, 0.4994, 0.4811],\n",
       "         [0.4979, 0.5083, 0.5158, 0.5056, 0.4749, 0.4902, 0.5048, 0.5128,\n",
       "          0.4905, 0.4834, 0.5057, 0.5160, 0.4824, 0.4856, 0.4871, 0.4987,\n",
       "          0.5156, 0.5159, 0.5200, 0.5183, 0.4997, 0.5083, 0.4931, 0.5182,\n",
       "          0.5174, 0.5035, 0.5042, 0.5138],\n",
       "         [0.4811, 0.4797, 0.4886, 0.5152, 0.5120, 0.4959, 0.5066, 0.5124,\n",
       "          0.4793, 0.5108, 0.5009, 0.5172, 0.5210, 0.5041, 0.4930, 0.5104,\n",
       "          0.4858, 0.5026, 0.5093, 0.5077, 0.4912, 0.4867, 0.5104, 0.4901,\n",
       "          0.5153, 0.5126, 0.5118, 0.5205],\n",
       "         [0.5057, 0.5093, 0.5120, 0.5111, 0.5239, 0.4878, 0.5082, 0.4955,\n",
       "          0.4892, 0.5048, 0.4904, 0.4965, 0.4958, 0.5090, 0.5024, 0.4983,\n",
       "          0.4912, 0.5137, 0.4729, 0.5146, 0.5028, 0.4930, 0.4803, 0.4951,\n",
       "          0.4881, 0.4980, 0.4868, 0.4831],\n",
       "         [0.5103, 0.5016, 0.5105, 0.5093, 0.5052, 0.5288, 0.5206, 0.5275,\n",
       "          0.4830, 0.4851, 0.4869, 0.5059, 0.5039, 0.4999, 0.5043, 0.4862,\n",
       "          0.4931, 0.5131, 0.5101, 0.5088, 0.5035, 0.4908, 0.4976, 0.5112,\n",
       "          0.5022, 0.4920, 0.4942, 0.4885],\n",
       "         [0.4747, 0.5119, 0.4876, 0.5094, 0.4839, 0.4842, 0.5158, 0.5073,\n",
       "          0.4993, 0.4865, 0.4951, 0.4934, 0.4983, 0.4921, 0.5068, 0.4753,\n",
       "          0.4953, 0.4941, 0.4850, 0.5053, 0.4987, 0.5023, 0.5239, 0.4895,\n",
       "          0.4787, 0.5218, 0.4957, 0.4882],\n",
       "         [0.5047, 0.4931, 0.4790, 0.5086, 0.5032, 0.5086, 0.4964, 0.4776,\n",
       "          0.5069, 0.5062, 0.4895, 0.4978, 0.5019, 0.4819, 0.5199, 0.5123,\n",
       "          0.5170, 0.5090, 0.4861, 0.4779, 0.4959, 0.5000, 0.4844, 0.5139,\n",
       "          0.5120, 0.4876, 0.5066, 0.4993],\n",
       "         [0.5201, 0.4897, 0.4931, 0.4962, 0.5329, 0.4802, 0.4986, 0.4887,\n",
       "          0.5162, 0.4839, 0.4951, 0.4974, 0.4969, 0.4821, 0.5081, 0.4854,\n",
       "          0.5018, 0.5069, 0.4936, 0.5084, 0.5041, 0.4864, 0.5200, 0.4951,\n",
       "          0.5039, 0.5164, 0.4994, 0.4991],\n",
       "         [0.5054, 0.4646, 0.5035, 0.4909, 0.4989, 0.4967, 0.5122, 0.5223,\n",
       "          0.5160, 0.5031, 0.4799, 0.4946, 0.4819, 0.4906, 0.4829, 0.4959,\n",
       "          0.4895, 0.5010, 0.5084, 0.5023, 0.5078, 0.4774, 0.4914, 0.4762,\n",
       "          0.4924, 0.5158, 0.5136, 0.4932],\n",
       "         [0.4921, 0.4786, 0.4973, 0.4908, 0.5108, 0.4968, 0.5080, 0.4864,\n",
       "          0.5145, 0.5135, 0.4923, 0.4965, 0.4941, 0.5216, 0.5103, 0.5123,\n",
       "          0.4819, 0.4824, 0.5021, 0.5160, 0.4936, 0.5100, 0.5065, 0.4849,\n",
       "          0.5041, 0.5103, 0.4777, 0.5161],\n",
       "         [0.4973, 0.4892, 0.4889, 0.4897, 0.4886, 0.4811, 0.4969, 0.4961,\n",
       "          0.5044, 0.5152, 0.5074, 0.4933, 0.5106, 0.5178, 0.4960, 0.4811,\n",
       "          0.5073, 0.4883, 0.5149, 0.4922, 0.5087, 0.4950, 0.5171, 0.5036,\n",
       "          0.4833, 0.5052, 0.4930, 0.4963],\n",
       "         [0.4963, 0.5014, 0.4971, 0.4901, 0.5001, 0.5001, 0.4879, 0.5051,\n",
       "          0.5068, 0.5052, 0.5001, 0.4972, 0.4676, 0.5120, 0.5099, 0.4858,\n",
       "          0.4991, 0.4982, 0.5237, 0.4904, 0.5208, 0.4790, 0.4970, 0.5135,\n",
       "          0.5046, 0.5109, 0.4840, 0.4855],\n",
       "         [0.4812, 0.5083, 0.4990, 0.4779, 0.5068, 0.5200, 0.4914, 0.4881,\n",
       "          0.5027, 0.4956, 0.5164, 0.4927, 0.4783, 0.4975, 0.4995, 0.4963,\n",
       "          0.5062, 0.5218, 0.5034, 0.4955, 0.4976, 0.5026, 0.4867, 0.4859,\n",
       "          0.4770, 0.5127, 0.5210, 0.5228],\n",
       "         [0.5105, 0.4793, 0.5058, 0.4967, 0.4920, 0.4991, 0.4796, 0.4953,\n",
       "          0.5214, 0.5173, 0.4782, 0.4773, 0.5003, 0.5063, 0.4992, 0.4984,\n",
       "          0.4974, 0.5122, 0.4885, 0.5091, 0.4841, 0.5060, 0.5070, 0.4942,\n",
       "          0.4891, 0.5118, 0.4738, 0.4808],\n",
       "         [0.5153, 0.4853, 0.5252, 0.4884, 0.4947, 0.4958, 0.4803, 0.5013,\n",
       "          0.4913, 0.5100, 0.4960, 0.4982, 0.4917, 0.4937, 0.4975, 0.4829,\n",
       "          0.4956, 0.5069, 0.5038, 0.5070, 0.4902, 0.5092, 0.5003, 0.5085,\n",
       "          0.5035, 0.4926, 0.4987, 0.4947],\n",
       "         [0.4961, 0.4829, 0.4956, 0.5074, 0.4817, 0.5034, 0.5131, 0.5075,\n",
       "          0.4950, 0.5103, 0.4984, 0.4812, 0.5204, 0.5012, 0.5042, 0.4954,\n",
       "          0.5135, 0.4948, 0.4962, 0.4997, 0.4940, 0.4999, 0.5028, 0.4963,\n",
       "          0.5109, 0.5039, 0.4976, 0.4992],\n",
       "         [0.5093, 0.5119, 0.5113, 0.4916, 0.4951, 0.5058, 0.5199, 0.5035,\n",
       "          0.5169, 0.5189, 0.5055, 0.4872, 0.5209, 0.5224, 0.5087, 0.4878,\n",
       "          0.4971, 0.4899, 0.5210, 0.4875, 0.5116, 0.5195, 0.5117, 0.5041,\n",
       "          0.5082, 0.4991, 0.5089, 0.5240],\n",
       "         [0.4800, 0.4959, 0.5081, 0.5292, 0.4838, 0.5085, 0.5058, 0.4903,\n",
       "          0.4889, 0.5112, 0.5122, 0.4741, 0.5098, 0.4791, 0.4856, 0.5126,\n",
       "          0.4980, 0.5041, 0.4891, 0.4995, 0.5035, 0.5324, 0.4908, 0.4995,\n",
       "          0.5135, 0.5122, 0.4860, 0.5096],\n",
       "         [0.4733, 0.5138, 0.5215, 0.4952, 0.4803, 0.4964, 0.4914, 0.4883,\n",
       "          0.4945, 0.5106, 0.5037, 0.4970, 0.5168, 0.5064, 0.5145, 0.4913,\n",
       "          0.5088, 0.5116, 0.5079, 0.5116, 0.4793, 0.4864, 0.5156, 0.4808,\n",
       "          0.4659, 0.5080, 0.5092, 0.5045]]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f6a4cb30-c344-4cab-9fe2-74dad6284e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(predict[1].detach().numpy()[0], cmap='gray')  # 흑백 이미지인 경우 cmap='gray' 사용\n",
    "plt.axis('off')\n",
    "plt.title('Generated Image')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc228f3-4132-4872-bd54-37f359b0dbd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nina",
   "language": "python",
   "name": "nina"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
